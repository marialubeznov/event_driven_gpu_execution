
machine(GPUL1Cache, "VI GPU L1 Cache")
: Sequencer * sequencer;
  CacheMemory * cache;
  int l2_select_num_bits;
  int num_l2;
  Cycles issue_latency := 2;


   // NETWORK BUFFERS
  MessageBuffer * requestFromL1Cache, network="To", virtual_network="7",
        vnet_type="request";

  MessageBuffer * responseToL1Cache, network="From", virtual_network="6",
        vnet_type="response";

{
  // STATES
  state_declaration(State, desc="Cache states") {
    I, AccessPermission:Invalid, desc="Not Present/Invalid";
    V, AccessPermission:Read_Only, desc="Valid";

    IA, AccessPermission:Busy, desc="Invalid, but waiting for ack or data from L2";
    IV, AccessPermission:Busy, desc="Issued request for LOAD/IFETCH";

    I_a, AccessPermission:Busy, desc="Issued atomic, waiting for data resp";
  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // From processor

    Load,       desc="Load request from processor";
    Ifetch,     desc="Ifetch request from processor";
    Store,      desc="Store request from processor";
    Flush_line, desc="Invalidate the line if valid";
    FlashInv,   desc="Invalidate the line if valid";

    BypassLoad, desc="Just like load, but we don't allocate a line";

    Atomic,     desc="Atomic request from processor";

    Data,       desc="Data from network";

    Replacement,  desc="Replace a block";
    Write_Ack,  desc="Ack from the directory for a writeback";
  }

  enumeration(RequestType, desc="Type of request for each transition") {
    DataArrayRead,    desc="L1 Data array read";
    DataArrayWrite,   desc="L1 Data array write";
    TagArrayRead,     desc="L1 Tag array read";
    TagArrayWrite,    desc="L1 Tag array write";
  }

  // STRUCTURE DEFINITIONS

  MessageBuffer mandatoryQueue;

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry") {
    State CacheState,        desc="cache state";
    bool Dirty,              desc="Is the data dirty (different than memory)?";
    DataBlock DataBlk,       desc="Data in the block";
  }


  // TBE fields
  structure(TBE, desc="...") {
    State TBEState,          desc="Transient state";
    DataBlock DataBlk,       desc="data for the block, required for concurrent writebacks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
  }


  // STRUCTURES

  TBETable TBEs, template="<GPUL1Cache_TBE>", constructor="m_number_of_TBEs";

  // needed for writeCallback to work. The data stored here is ignored
  DataBlock temp_store_data;

  // PROTOTYPES
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE b);
  void unset_tbe();
  void wakeUpAllBuffers();
  void wakeUpBuffers(Addr a);
  void profileMsgDelay(int virtualNetworkType, Cycles b);

  Entry getCacheEntry(Addr address), return_by_pointer="yes" {
    return static_cast(Entry, "pointer", cache.lookup(address));
  }


  int l2_select_low_bit, default="RubySystem::getBlockSizeBits()";

  // External functions
  MachineID getL2ID(Addr num, int num_l2s, int select_bits, int select_start_bit);

  // FUNCTIONS
  Event mandatory_request_type_to_event(RubyRequestType type) {
   if (type == RubyRequestType:LD) {
      return Event:Load;
    } else if (type == RubyRequestType:LD_Bypass) {
      return Event:BypassLoad;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:Ifetch;
    } else if (type == RubyRequestType:ST || type == RubyRequestType:ST_Bypass) {
      // In VI_hammer, stores bypass the L1, so ST_Bypass is same as ST
      return Event:Store;
    } else if ((type == RubyRequestType:FLUSH)) {
      return Event:Flush_line;
    } else if (type == RubyRequestType:ATOMIC) {
      return Event:Atomic;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  State getState(TBE tbe, Entry cache_entry, Addr addr) {

    if (is_valid(tbe)) {
      return tbe.TBEState;
    }
    else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    else {
      return State:I;
    }
  }

  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {

    if (is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  AccessPermission getAccessPermission(Addr addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      return GPUL1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      return GPUL1Cache_State_to_permission(cache_entry.CacheState);
    }

    return AccessPermission:NotPresent;
  }

  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(GPUL1Cache_State_to_permission(state));
    }
  }

  void functionalRead(Addr addr, Packet *pkt) {
    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      testAndRead(addr, cache_entry.DataBlk, pkt);
    } else {
      TBE tbe := TBEs[addr];
      if(is_valid(tbe)) {
        testAndRead(addr, tbe.DataBlk, pkt);
      } else {
        error("Missing data block");
      }
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    int num_functional_writes := 0;

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      num_functional_writes := num_functional_writes +
        testAndWrite(addr, cache_entry.DataBlk, pkt);
      return num_functional_writes;
    }

    TBE tbe := TBEs[addr];
    num_functional_writes := num_functional_writes +
      testAndWrite(addr, tbe.DataBlk, pkt);
    return num_functional_writes;
  }

  void recordRequestType(RequestType type, Addr addr) {
    if (type == RequestType:DataArrayRead) {
      cache.recordRequestType(CacheRequestType:DataArrayRead, addr);
    } else if (type == RequestType:DataArrayWrite) {
      cache.recordRequestType(CacheRequestType:DataArrayWrite, addr);
    } else if (type == RequestType:TagArrayRead) {
      cache.recordRequestType(CacheRequestType:TagArrayRead, addr);
    } else if (type == RequestType:TagArrayWrite) {
      cache.recordRequestType(CacheRequestType:TagArrayWrite, addr);
    } else {
      error("Bad request type passed to recordRequestType");
    }
  }

  bool checkResourceAvailable(RequestType type, Addr addr) {
    if (type == RequestType:DataArrayRead) {
      return cache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (type == RequestType:DataArrayWrite) {
      return cache.checkResourceAvailable(CacheResourceType:DataArray, addr);
    } else if (type == RequestType:TagArrayRead) {
      return cache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else if (type == RequestType:TagArrayWrite) {
      return cache.checkResourceAvailable(CacheResourceType:TagArray, addr);
    } else {
      error("Bad request type passed to checkResourceAvailable");
    }
  }

  // NETWORK PORTS

  out_port(requestNetwork_out, RequestMsgVI, requestFromL1Cache);

  in_port(responseNetwork_in, ResponseMsgVI, responseToL1Cache) {
    if (responseNetwork_in.isReady()) {
      peek(responseNetwork_in, ResponseMsgVI, block_on="addr") {

        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if (in_msg.Type == CoherenceResponseTypeVI:DATA) {
          //
          // NOTE: This implements late allocation of a cache data array frame
          // and is possible since GPU L1 caches do not hold dirty data that
          // may need to be written back. If this changes, the Replacement
          // trigger will need to be moved back to the request side for eager
          // cache frame allocation. 
          //
          if (cache.cacheAvail(in_msg.addr) == false) {
            trigger(Event:Replacement, cache.cacheProbe(in_msg.addr),
                    getCacheEntry(cache.cacheProbe(in_msg.addr)),
                    TBEs[cache.cacheProbe(in_msg.addr)]);
          } else {
            trigger(Event:Data, in_msg.addr, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseTypeVI:WB_ACK) {
          trigger(Event:Write_Ack, in_msg.addr, cache_entry, tbe);
        } else {
          error("Unexpected message");
        }
      }
    }
  }

    // Mandatory Queue
  in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...") {
    if (mandatoryQueue_in.isReady()) {
      peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {
        Entry cache_entry := getCacheEntry(in_msg.LineAddress);

        if (in_msg.Type == RubyRequestType:FLUSHALL) {
          trigger(Event:FlashInv, in_msg.LineAddress, cache_entry,
                  TBEs[in_msg.LineAddress]);
        } else {
          trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                  cache_entry, TBEs[in_msg.LineAddress]);
        }
      }
    }
  }

  // ACTIONS

  action(a_issueRequest, "a", desc="Issue a request") {
    enqueue(requestNetwork_out, RequestMsgVI, issue_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestTypeVI:GET;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(getL2ID(address, num_l2, l2_select_num_bits, l2_select_low_bit));
      out_msg.MessageSize := MessageSizeType:Control;
    }
  }

  action(b_issuePUT, "b", desc="Issue a PUT request") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestNetwork_out, RequestMsgVI, issue_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestTypeVI:PUT;
        out_msg.Requestor := machineID;
        out_msg.Destination.add(getL2ID(address, num_l2, l2_select_num_bits, l2_select_low_bit));
        out_msg.MessageSize := MessageSizeType:Data;
        // must write the data to the message so the L2 will have the right data
        in_msg.writeData(out_msg.DataBlk);
        out_msg.Offset := getOffset(in_msg.PhysicalAddress);
        out_msg.Size := in_msg.Size;
        DPRINTF(RubySlicc, "%s: offset: %d, size: %d\n", address, out_msg.Offset, out_msg.Size);
      }
    }
  }

  action(i_allocateL1CacheBlock, "c", desc="Allocate a cache block") {
    if (is_valid(cache_entry)) {
    } else {
      set_cache_entry(cache.allocate(address, new Entry));
    }
  }

  action(h_deallocateL1CacheBlock, "d", desc="deallocate a cache block") {
    if (is_valid(cache_entry)) {
      cache.deallocate(address);
      unset_cache_entry();
    }
  }

  action(m_popMandatoryQueue, "e", desc="Pop the mandatory request queue") {
    mandatoryQueue_in.dequeue();
  }

  action(n_popResponseQueue, "f", desc="Pop the response queue") {
    profileMsgDelay(1, responseNetwork_in.dequeue());
  }

  action(p_profileMiss, "g", desc="Profile cache miss") {
    ++cache.demand_misses;
  }

  action(q_profileHit, "q", desc="...") {
    ++cache.demand_hits;
  }

  action(r_load_hit, "h", desc="Notify sequencer the load completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
    sequencer.readCallback(address, cache_entry.DataBlk, false,
                           MachineType:GPUL1Cache);
  }

  action(rx_load_hit, "rx", desc="External load completed.") {
    peek(responseNetwork_in, ResponseMsgVI) {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
      sequencer.readCallback(address, cache_entry.DataBlk, true,
                             machineIDToMachineType(in_msg.Sender));
    }
  }

  action(rb_load_hit, "rb", desc="Bypass load completed.") {
    peek(responseNetwork_in, ResponseMsgVI) {
      assert(is_valid(tbe));
      tbe.DataBlk := in_msg.DataBlk;
      DPRINTF(RubySlicc,"%s\n", tbe.DataBlk);
      sequencer.readCallback(address, tbe.DataBlk, true,
                             machineIDToMachineType(in_msg.Sender));
    }
  }

  action(s_store_hit, "i", desc="Notify sequencer that store completed.") {
    // To make Ruby happy. We already wrote the data to L2 in b_issuePUT
    sequencer.writeCallback(address, temp_store_data, false,
                            MachineType:GPUL1Cache);
    DPRINTF(RubySlicc,"%s %s\n", address, temp_store_data);
  }

  action(sa_atomic_hit, "sa", desc="Notify sequencer that atomic completed.") {
    peek(responseNetwork_in, ResponseMsgVI) {
      // We already gained exclusive L2 access in b_issuePUT, so all that is
      // left is to handle the atomic in the hit callback
      sequencer.writeCallback(address, tbe.DataBlk, true,
                              machineIDToMachineType(in_msg.Sender));
      DPRINTF(RubySlicc,"ATOMIC: %s %s\n", address, temp_store_data);
    }
  }

  action(u_writeDataToCache, "j", desc="Write data to the cache") {
    peek(responseNetwork_in, ResponseMsgVI) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
    }
  }

  action(v_allocateTBE, "k", desc="Allocate TBE") {
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
  }


  action(w_deallocateTBE, "l", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(f_flashInv, "fi", desc="Invalidate all lines in the cache") {
    cache.flashInvalidate();
  }

  action(fr_flashInvEesp, "fr", desc="Ack the controller that flash inv is done") {
    sequencer.writeCallback(address, tbe.DataBlk, false, MachineType:GPUL1Cache);
  }

  action(zz_stallAndWaitMandatoryQueue, "\z", desc="Send the head of the mandatory queue to the back of the queue.") {
    stall_and_wait(mandatoryQueue_in, address);
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(ka_wakeUpAllDependents, "ka", desc="wake-up all dependents") {
    wakeUpAllBuffers();
  }

  // TRANSITIONS

  transition({IV, IA}, {Load, Ifetch, Store, BypassLoad, Flush_line, Replacement, Atomic}) {} {
    zz_stallAndWaitMandatoryQueue;
  }

  transition(V, Store, IA) {TagArrayRead, TagArrayWrite} {
    p_profileMiss;
    v_allocateTBE;
    b_issuePUT;
    h_deallocateL1CacheBlock;
    ka_wakeUpAllDependents;
    m_popMandatoryQueue;
  }

  transition(I, Store, IA) {TagArrayRead} {
    p_profileMiss;
    v_allocateTBE;
    b_issuePUT;
    m_popMandatoryQueue;
  }

  transition(V, Atomic, I_a) {TagArrayRead, TagArrayWrite} {
    p_profileMiss;
    v_allocateTBE;
    b_issuePUT;
    h_deallocateL1CacheBlock;
    ka_wakeUpAllDependents;
    m_popMandatoryQueue;
  }

  transition(I, Atomic, I_a) {TagArrayRead} {
    p_profileMiss;
    v_allocateTBE;
    b_issuePUT;
    m_popMandatoryQueue;
  }

  transition(V, {Load, Ifetch}) {TagArrayRead, DataArrayRead} {
    q_profileHit;
    r_load_hit;
    m_popMandatoryQueue;
  }

  transition(V, BypassLoad, IA) {TagArrayRead, TagArrayWrite, DataArrayRead} {
    p_profileMiss;
    v_allocateTBE;
    h_deallocateL1CacheBlock;
    a_issueRequest;
    m_popMandatoryQueue;
  }

  transition(I, {Load, Ifetch}, IV) {TagArrayRead} {
    p_profileMiss;
    v_allocateTBE;
    a_issueRequest;
    m_popMandatoryQueue;
  }

  transition(V, Replacement, I) {} {
    h_deallocateL1CacheBlock;
  }

  transition(I, BypassLoad, IA) {TagArrayRead} {
    p_profileMiss;
    v_allocateTBE;
    a_issueRequest;
    m_popMandatoryQueue;
  }

  transition(IV, Data, V) {TagArrayWrite, DataArrayWrite} {
    i_allocateL1CacheBlock;
    u_writeDataToCache;
    rx_load_hit;
    w_deallocateTBE;
    ka_wakeUpAllDependents;
    n_popResponseQueue;
  }

  transition(IA, Write_Ack, I) {} {
    s_store_hit;
    w_deallocateTBE;
    n_popResponseQueue;
  }

  transition(I_a, Write_Ack, I) {} {
    sa_atomic_hit;
    w_deallocateTBE;
    n_popResponseQueue;
  }

  transition(IA, Data, I) {
    rb_load_hit;
    w_deallocateTBE;
    n_popResponseQueue;
    ka_wakeUpAllDependents;
  }

  transition(V, Flush_line, I) {TagArrayRead, TagArrayWrite} {
    h_deallocateL1CacheBlock;
    ka_wakeUpAllDependents;
    m_popMandatoryQueue;
  }

  transition(I, Flush_line) {TagArrayRead} {
    m_popMandatoryQueue;
  }

  transition({I,V}, FlashInv, I) {TagArrayWrite} {
    f_flashInv;
    fr_flashInvEesp;
    m_popMandatoryQueue;
  }

}

